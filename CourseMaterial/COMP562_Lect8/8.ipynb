{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# COMP 562 – Lecture 8\n",
    "$$\n",
    "\\renewcommand{\\xx}{\\mathbf{x}}\n",
    "\\renewcommand{\\yy}{\\mathbf{y}}\n",
    "\\renewcommand{\\zz}{\\mathbf{z}}\n",
    "\\renewcommand{\\vv}{\\mathbf{v}}\n",
    "\\renewcommand{\\bbeta}{\\boldsymbol{\\mathbf{\\beta}}}\n",
    "\\renewcommand{\\mmu}{\\boldsymbol{\\mathbf{\\mu}}}\n",
    "\\renewcommand{\\ssigma}{\\boldsymbol{\\mathbf{\\sigma}}}\n",
    "\\renewcommand{\\reals}{\\mathbb{R}}\n",
    "\\renewcommand{\\loglik}{\\mathcal{LL}}\n",
    "\\renewcommand{\\penloglik}{\\mathcal{PLL}}\n",
    "\\renewcommand{\\likelihood}{\\mathcal{L}}\n",
    "\\renewcommand{\\Data}{\\textrm{Data}}\n",
    "\\renewcommand{\\given}{ | }\n",
    "\\renewcommand{\\MLE}{\\textrm{MLE}}\n",
    "\\renewcommand{\\tth}{\\textrm{th}}\n",
    "\\renewcommand{\\Gaussian}[2]{\\mathcal{N}\\left(#1,#2\\right)}\n",
    "\\renewcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\renewcommand{\\ones}{\\mathbf{1}}\n",
    "\\renewcommand{\\diag}[1]{\\textrm{diag}\\left( #1 \\right)}\n",
    "\\renewcommand{\\sigmoid}[1]{\\sigma\\left(#1\\right)}\n",
    "\\renewcommand{\\myexp}[1]{\\exp\\left\\{#1\\right\\}}\n",
    "\\renewcommand{\\mylog}[1]{\\log\\left\\{#1\\right\\}}\n",
    "\\renewcommand{\\argmax}{\\mathop{\\textrm{argmax}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multivariate Gaussian Distribution -- Dependent Case\n",
    "\n",
    "Suppose we have $p$ standard random variables  (0 mean, unit variance)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z_i \\sim& \\Gaussian{0}{1},&  i=1,\\dots p\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and we are given a vector $\\mmu$ of length $n$ and a full-rank matrix $A$ of size $p \\times p$\n",
    "\n",
    "Distribution of $\\xx = A\\zz + \\mu$ is\n",
    "$$\n",
    "p(\\xx) = \\left(2\\pi\\right)^{-\\frac{p}{2}}(\\det{\\Sigma})^{-\\frac{1}{2}}\\myexp{\\frac{1}{2}(\\xx - \\mmu)^T\\Sigma^{-1}(\\xx-\\mmu)}\n",
    "$$\n",
    "where $\\Sigma = AA^T$.\n",
    "\n",
    "* $\\mu$ is **mean** of the Gaussian\n",
    "* $\\Sigma$ is **covariance** matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Maximum Likelihood Estimates of Mean and Covariance\n",
    "\n",
    "Given data $\\{\\xx_i \\in \\reals^N|i=1,\\dots,N\\}$ maximum likelihood estimates (MLE) of mean and covariance are:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mmu^{\\MLE} &= \\frac{1}{N}\\sum_{i=1}^N \\xx_i\\\\\n",
    "\\Sigma^{\\MLE} & = \\frac{1}{N}\\sum_{i=1}^N \\underbrace{\\left(\\xx_i - \\mmu^{\\MLE}\\right)\\left(\\xx_i - \\mmu^{\\MLE}\\right)^T}_{\\textrm{ a matrix of size $p \\times p$}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Dimensionality\n",
    "* $\\mmu^{\\MLE}$ is of same dimension as a single data point $p \\times 1$.\n",
    "* $\\Sigma^{\\MLE}$ is a matrix of size $p \\times p$ \n",
    "\n",
    "Note that $\\xx\\xx^T$ and $\\xx^T\\xx$ are not the same, former is a matrix, latter is a scalar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generative Models for Classification \n",
    "\n",
    "There are two ways to factorize joint probability of labels and features\n",
    "\n",
    "$$\n",
    "p(y,\\xx\\given\\theta) = p(y\\given\\xx,\\theta)p(\\xx\\given\\theta) =  p(\\xx\\given y,\\theta)p(y\\given\\theta) \n",
    "$$\n",
    "\n",
    "The second one given us a simple process to *GENERATE* data:\n",
    "\n",
    "1. First select label according $p(y\\given\\theta)$, say it was $c$\n",
    "2. Now generate features $p(\\xx\\given y=c,\\theta)$\n",
    "\n",
    "Once we have such a model we can obtain the conditional probability $p(y\\given\\xx)$ using Bayes rule\n",
    "\n",
    "$$\n",
    "p(y=c\\given \\xx) = \\frac{p(y=c\\given\\theta)p(\\xx\\given y=c,\\theta)}{\\sum_k p(y=k\\given\\theta)p(\\xx\\given y=k,\\theta)}\n",
    "$$\n",
    "\n",
    "and we can predict label for a new feature vector $\\xx$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "$$\n",
    "p(\\xx\\given y,\\theta) = \\prod_j p(x_j \\given y, \\theta)\n",
    "$$\n",
    "\n",
    "This assumption **Conditional Independence of Features** underlies the **Naive Bayes** method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Naive Bayes\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(y = c \\given \\pi ) &= \\pi_c \\\\\n",
    "p(\\xx \\given y=c, \\theta) &= \\prod_j p(x_j \\given y= c,\\theta_{j,c})\n",
    "\\end{aligned}\n",
    "$$\n",
    "Parameters are \n",
    "* $\\pi_c$ prior probability that a sample comes from the class $c$\n",
    "* $\\theta_{j,c}$ parameters for the $j^\\tth$ feature for class $c$\n",
    "\n",
    "In general, there are many variants of Naive Bayes, you can choose different distributions for $p(x_j \\given y = c)$\n",
    "* Gaussian -- continuous features\n",
    "* Bernoulli -- binary features\n",
    "* Binomial -- count of positive outcomes\n",
    "* Categorical -- discrete features\n",
    "* Multinomial -- count of particular discrete outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Naive Bayes with Gaussian Features\n",
    "\n",
    "We will assume that \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_j\\given y_c, \\theta &\\sim \\Gaussian{\\theta_{j,c}}{\\sigma^2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Each feature is Gaussian distributed around class specific mean and with shared spherical variance\n",
    "\n",
    "Let's take a look at the data we generated earlier and read-off these parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**joint** Log-likelihood\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\loglik(\\theta,\\pi\\given\\yy,X) & = \\sum_i \\log p(y_i,\\xx_i\\given\\theta,\\pi)  && \\textrm{definition of likelihood} \\\\\n",
    "& = \\sum_i \\log p(y_i\\given\\pi) + \\log p(\\xx_i\\given y_i,\\theta) && \\textrm{factorization } p(y,\\xx) = p(y) p(\\xx|y) \\\\\n",
    "& = \\sum_i \\log p(y_i\\given\\pi) + \\log \\prod_j p(x_{j,i}\\given y_i,\\theta_j) && \\textrm{ Naive Bayes assumption}\\\\\n",
    "& = \\sum_i \\log p(y_i\\given\\pi) + \\sum_j \\log p(x_{j,i}\\given y_i,\\theta_j)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that we have not yet used our assumptions about distribution of $x_{j,i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning parameters for Naive Bayes with Gaussian features\n",
    "\n",
    "**joint** Log-likelihood\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\loglik(\\theta,\\pi\\given\\yy,X)  &= \\sum_i \\left[\\log p(y_i\\given\\pi) + \\sum_j \\log p(x_{j,i}\\given y_i,\\theta_j) \\right]\\\\\n",
    "& = \\sum_i \\left[\\log \\pi_{\\color{red}{y_i}} + \\sum_j \\log \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\myexp{-\\frac{1}{2\\sigma^2}(x_{j,i} - \\theta_{j,\\color{red}{y_i}})^2}\\right]\\\\\n",
    "& = \\sum_i \\left[\\log \\pi_{\\color{red}{y_i}} - \\frac{1}{2} \\sum_j \\log (2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_j (x_{j,i} - \\theta_{j,\\color{red}{y_i}})^2\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that parameters $\\pi_c$ and $\\theta_{i,c}$ are only used for samples that belong to class $c$ ($y_i=c$)\n",
    "\n",
    "Hence, we can learn of parameters for each class separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning Parameters for Naive Bayes with Gaussian Features\n",
    "\n",
    "Closed form estimates for parameters are\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\pi_c &= \\frac{\\sum_i [y_i = c]}{\\sum_i 1} && \\textrm{frequency of class $c$ in training data}\\\\\n",
    "\\theta_{j,c} &= \\frac{\\sum_i [y_i = c]x_{i,j}}{\\sum_i [y_i = c]} && \\textrm{average of feature $j$ among samples in class $c$}\\\\\n",
    "\\sigma &= \\frac{\\sum_i (x_{j,i} - \\theta_{j,y_i})^2}{\\sum_i 1}&& \\textrm{variance across all features}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note $[x]$ is an indicator function, defined as\n",
    "\n",
    "$$\n",
    "  [x] =\n",
    "  \\begin{cases}\n",
    "    1 & \\text{if $x$ is true} \\\\\n",
    "    0 & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Class Prediction using Naive Bayes with Gaussian Features\n",
    "\n",
    "Recall that \n",
    "$$\n",
    "\\mathop{\\textrm{argmax}}_c p(y=c\\given \\xx) = \n",
    "\\mathop{\\textrm{argmax}}_c \\log  p(y=c\\given\\theta) + \\log p(\\xx\\given y=c,\\theta)\n",
    "$$\n",
    "\n",
    "After a little bit more manipulation\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log p(y=c \\given \\xx,\\theta,\\pi) \n",
    "&= \\log \\pi_{c} - \\sum_j \\frac{1}{2\\sigma^2}(x_{j,i} - \\theta_{j,c})^2 + \\textrm{const.} \n",
    "\\end{aligned}\n",
    "$$\n",
    "Predicted class\n",
    "$$\n",
    "y^* = \\mathop{\\textrm{argmax}}_c \\log \\pi_{c} - \\underbrace{\\sum_j (x_{j,i} - \\theta_{j,c})^2}_{\\textrm{distance to class center}} + \\textrm{const.}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Larger $K$** means **less bias** towards overestimating the true expected error (as training folds will be closer to the total dataset) but **typically higher variance and higher running time** (as you are getting closer to the limit case: Leave-One-Out CV)\n",
    "* If cross-validation were averaging independent estimates, then with large $K$, one should see relatively lower variance between models; however, this is not true when training sets are highly correlated, which is what we typically deal with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification Performance -- Prediction Rate\n",
    "\n",
    "Sensitivity, Recall, or True Positive Rate (TPR) \n",
    "\n",
    "$$ \n",
    "TPR = \\frac{TP}{P} = \\frac{TP}{TP + FN} = 1 - FNR\n",
    "$$\n",
    "\n",
    "Specificity, Selectivity or True Negative Rate (TNR)\n",
    "\n",
    "$$ \n",
    "TNR = \\frac{TN}{N} = \\frac{TN}{TN + FP} = 1 - FPR\n",
    "$$\n",
    "\n",
    "False Positive Rate (FPR)\n",
    "$$ \n",
    "FPR = \\frac{FP}{N} = \\frac{FP}{TN + FP} = 1 - TNR\n",
    "$$\n",
    "\n",
    "False Negative Rate (FNR)\n",
    "$$ \n",
    "FNR = \\frac{FN}{P} = \\frac{FN}{TP + FN} = 1 - TPR\n",
    "$$\n",
    "\n",
    "These measures are computed from either +ve or -ve group, hence they don't depend on classes balance (prevalence = $\\frac{P}{P+N}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Precision or Positive Predictive Value (PPV)\n",
    "\n",
    "$$ \n",
    "PPV = \\frac{TP}{TP+FP}\n",
    "$$\n",
    "\n",
    "Negative Predictive Value (NPV)\n",
    "\n",
    "$$\n",
    "NPV = \\frac{TN}{TN+FN}\n",
    "$$\n",
    "\n",
    "Accuracy (ACC)\n",
    "\n",
    "$$\n",
    "ACC = \\frac{TP+TN}{P+N} = \\frac{TP+TN}{TP + FN + TN + FP} \n",
    "$$\n",
    "\n",
    "These measures are computed from both +ve and -ve groups, hence they depend on classes balance (prevalence = $\\frac{P}{P+N}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification Performance -- ROC Curves\n",
    "\n",
    "Predictions are based on a cutoff\n",
    "\n",
    "$$\n",
    "p(y=1|\\xx)>\\tau\n",
    "$$\n",
    "where $\\tau$ is typically 0.5\n",
    "\n",
    "This particular cutoff will result in a specific prediction rates; however, you may prefer to tradeoff false positives for false negatives -- health industry does"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification Performance -- ROC Curves\n",
    "<img src=\"./Images/roc_curve.png\"  width=\"600\" align=\"center\"/>\n",
    "\n",
    "* A good ROC curve – hugs top left corner: high TPR, low FPR\n",
    "* A bad ROC curve – runs along diagonal: TPR equals the FPR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probabilistic Interpretation of AUC\n",
    "\n",
    "**AUC Interpretation:** AUC is the probability of correct ranking of a random \"positive\"-\"negative\" pair\n",
    "\n",
    "* So, given a randomly chosen observation $x_{1}$ belonging to class 1, and a randomly chosen observation $x$ belonging to class 0, the AUC is the probability that the evaluated classification algorithm will assign a higher score to $x$ than to $x_{2}$, i.e., the conditional probability of $p(y=1|x_{1}) > p(y=1|x_{2})$\n",
    "\n",
    "**AUC Computation:** Among all \"positive\"-\"negative\" pairs in the dataset compute the proportion of those which are ranked correctly by the evaluated classification algorithm\n",
    "\n",
    "$$\n",
    "\\hat{AUC} = \\frac{1}{P \\times N} \\sum_{i = 1}^{P} \\sum_{j = 1}^{N} [p(y=1|x_{i}) > p(y=1|x_{j})]\n",
    "$$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
